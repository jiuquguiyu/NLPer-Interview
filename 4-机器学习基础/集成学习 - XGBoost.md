# 集成学习 - XGBoost

---

# 前言
# 1，Xgboost简介
　　Xgboost是Boosting算法的其中一种，Boosting算法的思想是将许多弱分类器集成在一起，形成一个强分类器。因为Xgboost是一种提升树模型，所以它是将许多树模型集成在一起，形成一个很强的分类器。而所用到的树模型则是CART回归树模型。

　　Xgboost是在GBDT的基础上进行改进，使之更强大，适用于更大范围。

　　Xgboost一般和sklearn一起使用，但是由于sklearn中没有集成Xgboost，所以才需要单独下载安装。

# 2，Xgboost的优点
　　Xgboost算法可以给预测模型带来能力的提升。当我们对其表现有更多了解的时候，我们会发现他有如下优势：

## 2.1  正则化
　　实际上，Xgboost是以“正则化提升（regularized boosting）” 技术而闻名。Xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数，每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是Xgboost优于传统GBDT的一个特征

## 2.2  并行处理
　　Xgboost工具支持并行。众所周知，Boosting算法是顺序处理的，也是说Boosting不是一种串行的结构吗？怎么并行的？注意Xgboost的并行不是tree粒度的并行。Xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含）。Xgboost的并行式在特征粒度上的，也就是说每一颗树的构造都依赖于前一颗树。

　　我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），Xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分类时，需要计算每个特征的增益，大大减少计算量。这个block结构也使得并行成为了可能，在进行节点的分裂的时候，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

## 2.3  灵活性
　　Xgboost支持用户自定义目标函数和评估函数，只要目标函数二阶可导就行。它对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。

## 2.4  缺失值处理
　　对于特征的值有缺失的样本，Xgboost可以自动学习出他的分裂方向。Xgboost内置处理缺失值的规则。用户需要提供一个和其他样本不同的值，然后把它作为一个参数穿进去，以此来作为缺失值的取值。Xgboost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。

## 2.5  剪枝
　　Xgboost先从顶到底建立所有可以建立的子树，再从底到顶反向机芯剪枝，比起GBM，这样不容易陷入局部最优解

## 2.6  内置交叉验证
　　Xgboost允许在每一轮Boosting迭代中使用交叉验证。因此可以方便的获得最优Boosting迭代次数，而GBM使用网格搜索，只能检测有限个值。
