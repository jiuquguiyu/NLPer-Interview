# 朴素贝叶斯
tags: machine-learning

---

[TOC]

## 1. 基本概念

### 1. 条件概率

$$
P(X|Y) =  \frac{P(X,Y)}{P(Y)}
$$
![bayes](https://github.com/jiuquguiyu/NLPer-Interview/blob/master/img/%E8%B4%9D%E5%8F%B6%E6%96%AF/bayes.png)

- $P(X|Y)$含义： 表示 y 发生的条件下 x 发生的概率。

### 2. 先验概率

- 含义： **表示事件发生前的预判概率。**这个可以是基于历史数据统计，也可以由背景常识得出，也可以是主观观点得出。一般都是单独事件发生的概率，如 P(A)

### 3. 后验概率

- 基于先验概率求得的**反向条件概率**，形式上与条件概率相同（若 `P(X|Y)` 为正向，则 `P(Y|X)` 为反向）

## 2. 贝叶斯公式

贝叶斯公式如下：
$$
P(Y|X) = \frac{P(X|Y) P(Y)}{P(X)}  \\
$$
> - P(Y) 叫做**先验概率**，意思是事件X发生之前，我们对事件Y发生的一个概率的判断
> - P(Y|X) 叫做**后验概率**，意思是时间X发生之后，我们对事件Y发生的一个概率的重新评估
> - P(Y,X) 叫做**联合概率**， 意思是事件X与事件Y同时发生的概率。

## 3. 条件独立假设

$$
P(x|c) = p(x_1, x_2,  \cdots x_n | c) = \prod_{i=1}^Np(x_i | c)
$$

朴素贝叶斯采用条件独立假设的动机在于： **简化运算。**

## 4. 从机器学习视角理解朴素贝叶斯

**朴素贝叶斯 = 贝叶斯方法 + 条件独立假设。**

在机器学习中，我们可以将 X 理解为“具有某特征”， 而 Y 理解为“类别标签”，于是有：
$$
P("属于某类“ \, \, | \, \, "具有某特征" ) = \frac{P("具有某特征" | "属于某类") P("属于某类")}{P("具有某特征" )}
$$

而贝叶斯派目的是要对 Y 进行优化：
$$
\begin{align}
Y_{MAP} &= argmax_y P(Y|X)  \\
&= argmax_y \frac{P(X,Y)}{P(X)} \\
&= argmax_y P(Y) P(X|Y)
\end{align}
$$

## 朴素贝叶斯中的三种模型

### 1.  多项式模型

多项式模型适用于离散特征情况，在文本领域应用广泛， 其基本思想是：**我们将重复的词语视为其出现多次**。

### 2. 高斯模型

https://blog.csdn.net/u012162613/article/details/48323777

http://www.letiantian.me/2014-10-12-three-models-of-naive-nayes/

高斯模型适合**连续特征情况**， 我们先给出高斯公式：
$$
P(x_{i}|y_{k}) = \frac{1}{\sqrt{2\pi\sigma_{y_{k}}^{2}}}exp( -\frac{(x_{i}-\mu_{y_{k}})^2}  {2\sigma_{y_{k}}^{2}}   )
$$


### 3. 伯努利模型

> 伯努利模型适用于离散特征情况，它将重复的词语都视为只出现一次。
> $$
> P( " 代开“， ”发票“， ”发票“， ”我“ | S) = P("代开" | S)   P( ”发票“ | S) P("我" | S)
> $$
> 我们看到，”发票“出现了两次，但是我们只将其算作一次。

---

## QA

### 1. 朴素贝叶斯为何朴素？

朴素贝叶斯的朴素性体现在该算法基于一个简单的假设： **所有的变量都是相互独立的**，用贝叶斯公式表达如下：
$$
P(Y|X_1, X_2) = \frac{P(X_1|Y) P(X_2|Y) P(Y)}{P(X_1)P(X_2)}
$$
**而在很多情况下，所有变量几乎不可能满足两两之间的条件。**

### 2. 朴素贝叶斯分类中某个类别的概率为0怎么办？

**问题：** 如下，A1,A2,A3是三个特征，Y是分类结果。

|  A1  |  A2  |  A3  |  Y   |
| :--: | :--: | :--: | :--: |
|  1   |  1   |  0   |  1   |
|  0   |  1   |  1   |  1   |
|  1   |  0   |  1   |  0   |
|  0   |  1   |  0   |  0   |
|  0   |  0   |  1   |  0   |

```
P(Y=0) = 3/5
P(Y=1) = 2/5
P(Y=0|A1=1,A2=0,A3=0) = 3/5 * 1/3 * 2/3 * 1/3 = 2/45
P(Y=1|A1=1,A2=0,A3=0) = 2/5 * 1/2 * 1/4 * 1/2 = 1/40
```

答案是 **拉普拉斯平滑**。

### 3. 朴素贝叶斯的要求是什么？

- 贝叶斯定理
- 特征条件独立假设

### 4. 朴素贝叶斯的优缺点？

- 优点： 对小规模数据表现很好，适合多分类任务，适合增量式训练。
- 缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。

### 5. 朴素贝叶斯与 LR 区别？

-  朴素贝叶斯是生成模型，根据已有样本进行贝叶斯估计学习出先验概率 P(Y) 和条件概率 P(X|Y)，进而求出联合分布概率 P(XY)，最后利用贝叶斯定理求解P(Y|X)， 而LR是判别模型，根据极大化对数似然函数直接求出条件概率 P(Y|X)
-  朴素贝叶斯是基于很强的**条件独立假设**（在已知分类Y的条件下，各个特征变量取值是相互独立的），而 LR 则对此没有要求
-  朴素贝叶斯适用于数据集少的情景，而LR适用于大规模数据集。


-----------------------------------------------------------------------------------

在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数𝑌=𝑓(𝑋),要么是条件分布𝑃(𝑌|𝑋)。但是朴素贝叶斯却是生成方法，也就是直接找出特征输出Y和特征X的联合分布𝑃(𝑋,𝑌),然后用𝑃(𝑌|𝑋)=𝑃(𝑋,𝑌)/𝑃(𝑋)得出。

　　　　朴素贝叶斯很直观，计算量也不大，在很多领域有广泛的应用，这里我们就对朴素贝叶斯算法原理做一个小结。

# 1. 朴素贝叶斯相关的统计学知识
　　　　在了解朴素贝叶斯的算法之前，我们需要对相关必须的统计学知识做一个回顾。

　　　　贝叶斯学派很古老，但是从诞生到一百年前一直不是主流。主流是频率学派。频率学派的权威皮尔逊和费歇尔都对贝叶斯学派不屑一顾，但是贝叶斯学派硬是凭借在现代特定领域的出色应用表现为自己赢得了半壁江山。

　　　　贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。数据大家好理解，被频率学派攻击的是先验概率，一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑里推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类。

　　　　我们先看看条件独立公式，如果X和Y相互独立，则有：

𝑃(𝑋,𝑌)=𝑃(𝑋)𝑃(𝑌)
　　　　我们接着看看条件概率公式：

𝑃(𝑌|𝑋)=𝑃(𝑋,𝑌)/𝑃(𝑋)
𝑃(𝑋|𝑌)=𝑃(𝑋,𝑌)/𝑃(𝑌)
或者说:

𝑃(𝑌|𝑋)=𝑃(𝑋|𝑌)𝑃(𝑌)/𝑃(𝑋)
接着看看全概率公式

𝑃(𝑋)=∑𝑘𝑃(𝑋|𝑌=𝑌𝑘)𝑃(𝑌𝑘)其中∑𝑘𝑃(𝑌𝑘)=1
从上面的公式很容易得出贝叶斯公式：

𝑃(𝑌𝑘|𝑋)=𝑃(𝑋|𝑌𝑘)𝑃(𝑌𝑘)∑𝑘𝑃(𝑋|𝑌=𝑌𝑘)𝑃(𝑌𝑘)
 # 2. 朴素贝叶斯的模型
　　　　从统计学知识回到我们的数据分析。假如我们的分类模型样本是：
(𝑥(1)1,𝑥(1)2,...𝑥(1)𝑛,𝑦1),(𝑥(2)1,𝑥(2)2,...𝑥(2)𝑛,𝑦2),...(𝑥(𝑚)1,𝑥(𝑚)2,...𝑥(𝑚)𝑛,𝑦𝑚)
　　　　即我们有m个样本，每个样本有n个特征，特征输出有K个类别，定义为𝐶1,𝐶2,...,𝐶𝐾。

　　　　从样本我们可以学习得到朴素贝叶斯的先验分布𝑃(𝑌=𝐶𝑘)(𝑘=1,2,...𝐾),接着学习到条件概率分布𝑃(𝑋=𝑥|𝑌=𝐶𝑘)=𝑃(𝑋1=𝑥1,𝑋2=𝑥2,...𝑋𝑛=𝑥𝑛|𝑌=𝐶𝑘),然后我们就可以用贝叶斯公式得到X和Y的联合分布P(X,Y)了。联合分布P(X,Y)定义为：

𝑃(𝑋,𝑌=𝐶𝑘)=𝑃(𝑌=𝐶𝑘)𝑃(𝑋=𝑥|𝑌=𝐶𝑘)=𝑃(𝑌=𝐶𝑘)𝑃(𝑋1=𝑥1,𝑋2=𝑥2,...𝑋𝑛=𝑥𝑛|𝑌=𝐶𝑘)(1)(2)
　　　　从上面的式子可以看出𝑃(𝑌=𝐶𝑘)比较容易通过最大似然法求出，得到的𝑃(𝑌=𝐶𝑘)就是类别𝐶𝑘在训练集里面出现的频数。但是𝑃(𝑋1=𝑥1,𝑋2=𝑥2,...𝑋𝑛=𝑥𝑛|𝑌=𝐶𝑘)很难求出,这是一个超级复杂的有n个维度的条件分布。朴素贝叶斯模型在这里做了一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出:

𝑃(𝑋1=𝑥1,𝑋2=𝑥2,...𝑋𝑛=𝑥𝑛|𝑌=𝐶𝑘)=𝑃(𝑋1=𝑥1|𝑌=𝐶𝑘)𝑃(𝑋2=𝑥2|𝑌=𝐶𝑘)...𝑃(𝑋𝑛=𝑥𝑛|𝑌=𝐶𝑘)
　　　　从上式可以看出，这个很难的条件分布大大的简化了，但是这也可能带来预测的不准确性。你会说如果我的特征之间非常不独立怎么办？如果真是非常不独立的话，那就尽量不要使用朴素贝叶斯模型了，考虑使用其他的分类方法比较好。但是一般情况下，样本的特征之间独立这个条件的确是弱成立的，尤其是数据量非常大的时候。虽然我们牺牲了准确性，但是得到的好处是模型的条件分布的计算大大简化了，这就是贝叶斯模型的选择。

　　　　最后回到我们要解决的问题，我们的问题是给定测试集的一个新样本特征(𝑥(𝑡𝑒𝑠𝑡)1,𝑥(𝑡𝑒𝑠𝑡)2,...𝑥(𝑡𝑒𝑠𝑡)𝑛，我们如何判断它属于哪个类型？

　　　　既然是贝叶斯模型，当然是后验概率最大化来判断分类了。我们只要计算出所有的K个条件概率𝑃(𝑌=𝐶𝑘|𝑋=𝑋(𝑡𝑒𝑠𝑡)),然后找出最大的条件概率对应的类别，这就是朴素贝叶斯的预测了。

# 3. 朴素贝叶斯的推断过程
　　　　上节我们已经对朴素贝叶斯的模型也预测方法做了一个大概的解释，这里我们对朴素贝叶斯的推断过程做一个完整的诠释过程。

　　　　我们预测的类别𝐶𝑟𝑒𝑠𝑢𝑙𝑡是使𝑃(𝑌=𝐶𝑘|𝑋=𝑋(𝑡𝑒𝑠𝑡))最大化的类别，数学表达式为：

𝐶𝑟𝑒𝑠𝑢𝑙𝑡=𝑎𝑟𝑔𝑚𝑎𝑥⏟𝐶𝑘𝑃(𝑌=𝐶𝑘|𝑋=𝑋(𝑡𝑒𝑠𝑡))=𝑎𝑟𝑔𝑚𝑎𝑥⏟𝐶𝑘𝑃(𝑋=𝑋(𝑡𝑒𝑠𝑡)|𝑌=𝐶𝑘)𝑃(𝑌=𝐶𝑘)/𝑃(𝑋=𝑋(𝑡𝑒𝑠𝑡))(3)(4)
　　　　由于对于所有的类别计算𝑃(𝑌=𝐶𝑘|𝑋=𝑋(𝑡𝑒𝑠𝑡))时，上式的分母是一样的，都是𝑃(𝑋=𝑋(𝑡𝑒𝑠𝑡)，因此，我们的预测公式可以简化为：

𝐶𝑟𝑒𝑠𝑢𝑙𝑡=𝑎𝑟𝑔𝑚𝑎𝑥⏟𝐶𝑘𝑃(𝑋=𝑋(𝑡𝑒𝑠𝑡)|𝑌=𝐶𝑘)𝑃(𝑌=𝐶𝑘)
　　　

　　　　接着我们利用朴素贝叶斯的独立性假设，就可以得到通常意义上的朴素贝叶斯推断公式:

𝐶𝑟𝑒𝑠𝑢𝑙𝑡=𝑎𝑟𝑔𝑚𝑎𝑥⏟𝐶𝑘𝑃(𝑌=𝐶𝑘)∏𝑗=1𝑛𝑃(𝑋𝑗=𝑋(𝑡𝑒𝑠𝑡)𝑗|𝑌=𝐶𝑘)
# 4. 朴素贝叶斯的参数估计
　　　　在上一节中，我们知道只要求出𝑃(𝑌=𝐶𝑘)和𝑃(𝑋𝑗=𝑋(𝑡𝑒𝑠𝑡)𝑗|𝑌=𝐶𝑘)(𝑗=1,2,...𝑛)，我们通过比较就可以得到朴素贝叶斯的推断结果。这一节我们就讨论怎么通过训练集计算这两个概率。

　　　　对于𝑃(𝑌=𝐶𝑘),比较简单，通过极大似然估计我们很容易得到𝑃(𝑌=𝐶𝑘)为样本类别𝐶𝑘出现的频率，即样本类别𝐶𝑘出现的次数𝑚𝑘除以样本总数m。

　　　　对于𝑃(𝑋𝑗=𝑋(𝑡𝑒𝑠𝑡)𝑗|𝑌=𝐶𝑘)(𝑗=1,2,...𝑛),这个取决于我们的先验条件：

 

　　　　a) 如果我们的𝑋𝑗是离散的值，那么我们可以假设𝑋𝑗符合多项式分布，这样得到𝑃(𝑋𝑗=𝑋(𝑡𝑒𝑠𝑡)𝑗|𝑌=𝐶𝑘) 是在样本类别𝐶𝑘中，特征𝑋(𝑡𝑒𝑠𝑡)𝑗出现的频率。即：

𝑃(𝑋𝑗=𝑋(𝑡𝑒𝑠𝑡)𝑗|𝑌=𝐶𝑘)=𝑚𝑘𝑗𝑡𝑒𝑠𝑡𝑚𝑘
　　　　其中𝑚𝑘为样本类别𝐶𝑘总的特征计数，而𝑚𝑘𝑗𝑡𝑒𝑠𝑡为类别为𝐶𝑘的样本中，第j维特征𝑋(𝑡𝑒𝑠𝑡)𝑗出现的计数。

　　　　某些时候，可能某些类别在样本中没有出现，这样可能导致𝑃(𝑋𝑗=𝑋(𝑡𝑒𝑠𝑡)𝑗|𝑌=𝐶𝑘)为0，这样会影响后验的估计，为了解决这种情况，我们引入了拉普拉斯平滑，即此时有：

𝑃(𝑋𝑗=𝑋(𝑡𝑒𝑠𝑡)𝑗|𝑌=𝐶𝑘)=𝑚𝑘𝑗𝑡𝑒𝑠𝑡+𝜆𝑚𝑘+𝑂𝑗𝜆
　　　

　　　　其中𝜆 为一个大于0的常数，常常取为1。𝑂𝑗为第j个特征的取值个数。

　

　　　　b)如果我们我们的𝑋𝑗是非常稀疏的离散值，即各个特征出现概率很低，这时我们可以假设𝑋𝑗符合伯努利分布，即特征𝑋𝑗出现记为1，不出现记为0。即只要𝑋𝑗出现即可，我们不关注𝑋𝑗的次数。这样得到𝑃(𝑋𝑗=𝑋(𝑡𝑒𝑠𝑡)𝑗|𝑌=𝐶𝑘) 是在样本类别𝐶𝑘中，𝑋(𝑡𝑒𝑠𝑡)𝑗出现的频率。此时有：

𝑃(𝑋𝑗=𝑋(𝑡𝑒𝑠𝑡)𝑗|𝑌=𝐶𝑘)=𝑃(𝑋𝑗=1|𝑌=𝐶𝑘)𝑋(𝑡𝑒𝑠𝑡)𝑗+(1−𝑃(𝑋𝑗=1|𝑌=𝐶𝑘))(1−𝑋(𝑡𝑒𝑠𝑡)𝑗)
　　　　其中，𝑋(𝑡𝑒𝑠𝑡)𝑗取值为0和1。

　　

　　　　c)如果我们我们的𝑋𝑗是连续值，我们通常取𝑋𝑗的先验概率为正态分布，即在样本类别𝐶𝑘中，𝑋𝑗的值符合正态分布。这样𝑃(𝑋𝑗=𝑋(𝑡𝑒𝑠𝑡)𝑗|𝑌=𝐶𝑘)的概率分布是：

𝑃(𝑋𝑗=𝑋(𝑡𝑒𝑠𝑡)𝑗|𝑌=𝐶𝑘)=12𝜋𝜎2𝑘‾‾‾‾‾√𝑒𝑥𝑝(−(𝑋(𝑡𝑒𝑠𝑡)𝑗−𝜇𝑘)22𝜎2𝑘)
　　　　其中𝜇𝑘和𝜎2𝑘是正态分布的期望和方差，可以通过极大似然估计求得。𝜇𝑘为在样本类别𝐶𝑘中，所有𝑋𝑗的平均值。𝜎2𝑘为在样本类别𝐶𝑘中，所有𝑋𝑗的方差。对于一个连续的样本值，带入正态分布的公式，就可以求出概率分布了。

# 5.  朴素贝叶斯算法过程
　　　　我们假设训练集为m个样本n个维度，如下：

(𝑥(1)1,𝑥(1)2,...𝑥(1)𝑛,𝑦1),(𝑥(2)1,𝑥(2)2,...𝑥(2)𝑛,𝑦2),...(𝑥(𝑚)1,𝑥(𝑚)2,...𝑥(𝑚)𝑛,𝑦𝑚)
　　　　共有K个特征输出类别，分别为𝐶1,𝐶2,...,𝐶𝐾,每个特征输出类别的样本个数为𝑚1,𝑚2,...,𝑚𝐾,在第k个类别中，如果是离散特征，则特征𝑋𝑗各个类别取值为𝑚𝑘𝑗𝑙。其中l取值为1,2,...𝑆𝑗，𝑆𝑗为特征j不同的取值数。

　　　　输出为实例𝑋(𝑡𝑒𝑠𝑡)的分类。

　　　　算法流程如下：

## 　　　　1) 如果没有Y的先验概率，则计算Y的K个先验概率：𝑃(𝑌=𝐶𝑘)=(𝑚𝑘+𝜆)/(𝑚+𝐾𝜆)，否则𝑃(𝑌=𝐶𝑘)为输入的先验概率。

## 　　　　2) 分别计算第k个类别的第j维特征的第l个个取值条件概率：𝑃(𝑋𝑗=𝑥𝑗𝑙|𝑌=𝐶𝑘)
　　　　　　a)如果是离散值:

𝑃(𝑋𝑗=𝑥𝑗𝑙|𝑌=𝐶𝑘)=𝑚𝑘𝑗𝑙+𝜆𝑚𝑘+𝑆𝑗𝜆
　　　　　　𝜆可以取值为1，或者其他大于0的数字。

　　　　　　b)如果是稀疏二项离散值:
𝑃(𝑋𝑗=𝑥𝑗𝑙|𝑌=𝐶𝑘)=𝑃(𝑗|𝑌=𝐶𝑘)𝑥𝑗𝑙+(1−𝑃(𝑗|𝑌=𝐶𝑘)(1−𝑥𝑗𝑙)
　　　　　　 此时𝑙只有两种取值。

　　　　　　c)如果是连续值不需要计算各个l的取值概率，直接求正态分布的参数:

𝑃(𝑋𝑗=𝑥𝑗|𝑌=𝐶𝑘)=12𝜋𝜎2𝑘‾‾‾‾‾√𝑒𝑥𝑝(−(𝑥𝑗−𝜇𝑘)22𝜎2𝑘)
　　　　　　需要求出𝜇𝑘和𝜎2𝑘。 𝜇𝑘为在样本类别𝐶𝑘中，所有𝑋𝑗的平均值。𝜎2𝑘为在样本类别𝐶𝑘中，所有𝑋𝑗的方差。

## 　　　　3）对于实例𝑋(𝑡𝑒𝑠𝑡)，分别计算：

𝑃(𝑌=𝐶𝑘)∏𝑗=1𝑛𝑃(𝑋𝑗=𝑥(𝑡𝑒𝑠𝑡)𝑗|𝑌=𝐶𝑘)
　## 　　　4）确定实例𝑋(𝑡𝑒𝑠𝑡)的分类𝐶𝑟𝑒𝑠𝑢𝑙𝑡
𝐶𝑟𝑒𝑠𝑢𝑙𝑡=𝑎𝑟𝑔𝑚𝑎𝑥⏟𝐶𝑘𝑃(𝑌=𝐶𝑘)∏𝑗=1𝑛𝑃(𝑋𝑗=𝑋(𝑡𝑒𝑠𝑡)𝑗|𝑌=𝐶𝑘)
 

　　　　 从上面的计算可以看出，没有复杂的求导和矩阵运算，因此效率很高。

# 6.  朴素贝叶斯算法小结
　　　　朴素贝叶斯算法的主要原理基本已经做了总结，这里对朴素贝叶斯的优缺点做一个总结。

　　　　朴素贝叶斯的主要优点有：

　## 　　　1）朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。

　## 　　　2）对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练。

　## 　　　3）对缺失数据不太敏感，算法也比较简单，常用于文本分类。

　# 　　朴素贝叶斯的主要缺点有：　　　

　## 　　　1） 理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型给定输出类别的情况下,假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。

　## 　　　2）需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。

　## 　　　3）由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。

　## 　　　4）对输入数据的表达形式很敏感。
 
