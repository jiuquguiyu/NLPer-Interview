# 模型 - KNN

---

[TOC]

## 简介

KNN算法是分类算法的一种，也属于监督学习算法，其基本思想为：

- 当输入一个新的样本时，将新数据的每个特征与样本集中每个样本数据的特征进行比较。

- 从样本集中选取最**相近**的 K 个样本，然后依据某种决策原则（少数服从多数）来判定这个新样本的 label。

在KNN算法中，有三个主要要素：距离度量，K 的取值，分类决策规则。

## 算法步骤

- 计算新数据与数据集中所有样本之间的距离， 距离度量公式可以选择多种，如欧式距离，曼哈顿距离等。可以参见：[基础理论 - 距离度量方法](./基础理论 - 距离度量方法.md)
- 按照距离，将样本按照距离值进行排序

- 选取与当前数据距离最小的 K 个点。
- 返回这 K 个点的 label， 依据某种决策原则来判定这个新数据的 label 。

邻近算法，或者说K最近邻(kNN，k-NearestNeighbor)分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。Cover和Hart在1968年提出了最初的邻近算法。KNN是一种分类(classification)算法，它输入基于实例的学习（instance-based learning），属于懒惰学习（lazy learning）即KNN没有显式的学习过程，也就是说没有训练阶段，数据集事先已有了分类和特征值，待收到新样本后直接进行处理。与急切学习（eager learning）相对应。

　　KNN是通过测量不同特征值之间的距离进行分类。 

　　思路是：如果一个样本在特征空间中的k个最邻近的样本中的大多数属于某一个类别，则该样本也划分为这个类别。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。

　　提到KNN，网上最常见的就是下面这个图，可以帮助大家理解。

　　我们要确定绿点属于哪个颜色（红色或者蓝色），要做的就是选出距离目标点距离最近的k个点，看这k个点的大多数颜色是什么颜色。当k取3的时候，我们可以看出距离最近的三个，分别是红色、红色、蓝色，因此得到目标点为红色。


## K 的选择

- 如果 K 较小，预测结果会对邻近的点十分敏感，如果邻近点恰好是噪声，则很容易预测错误。 换个角度说说， **K的减小意味着模型变得复杂，容易发生过拟合。**
- 如果K 较大， 此时不相似的样本也会对预测产生作用，使得预测发生错误。换个角度说，**K的增大意味着模型变得简单，容易发生欠拟合。**


---

## QA

### 1. KNN 中为何采用欧式距离而不采用曼哈顿距离？

我们不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制。另一方面，欧式距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向的运动。

### 2. 

